# EVA_B4_DNN_PyTorch_BN-GBN
## Batch Normalization and Ghost Batch Normalization
This Assignment emphasizes on the use of Batch Normalization and L1/L2 Regularization.
It has been explained through the code, different variations in the outcome after using or not using Batch Normalization
with L1/L2 Regularization.
This Assignment also introduces a technique called Ghost Batch Normalization, where we can get substantial improvement
without using additional resources.

It works on Mini Batches.
Ghost Batch Normalization, by decreasing the number of examples that the normalization statistics are calculated over, increases the strength of the stochasticity, thereby increasing the amount of regularization.
